# Default values for kserve.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- For release specific common labels
labels:
  common: {}
# -- Specify how inference service is deployed:
# - Serverless, use Knative
# - RawDeployment, use K8S Deployment
defaultDeploymentMode: Serverless
# controller deployment configuration
controller:
  image:
    registry: ""
    repository: kserve/kserve-controller
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v0.8.0"
  # -- These are example resource values set for the controller, please override accordingly
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 512Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
# -- Ingress configuration
ingress:
  # -- ingressGateway refers to the Istio Gateway resource name. Assumed
  # to be in knative-serving namespace by default.
  ingressGateway: knative-serving/knative-ingress-gateway
  # -- ingressService refers to the ingressgateway service name. Assumed to be in istio-system
  # namespace by default.
  ingressService: istio-ingressgateway.istio-system.svc.cluster.local
  # -- localGateway refers to the Istio Gateway resource name. Assumed to in
  # knative-serving namespace by default.
  localGateway: knative-serving/knative-local-gateway
  # -- ingressService refers to the local ingressgateway service name. Assumed to be in istio-system
  # namespace by default and called cluster-local-gateway.
  localGatewayService: cluster-local-gateway.istio-system.svc.cluster.local
  # -- domain used for inferenceservice deployments
  ingressDomain: example.com
  ingressClassName: istio
  # Format of deployments
  domainTemplate: "{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}"
  # -- Set default scheme of inferenceservice url, defaults to http if unset
  # urlScheme: "http"
logger:
  image:
    registry: ""
    repository: kserve/agent
    tag: v0.8.0
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  defaultUrl: http://default-broker
agent:
  image:
    registry: ""
    repository: kserve/agent
    tag: v0.8.0
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1000m
      memory: 1Gi
batcher:
  image:
    registry: ""
    repository: kserve/agent
    tag: v0.8.0
  resources:
    requests:
      cpu: 1000m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 1Gi
# storageInitializer container specs if enabled
storageInitializer:
  image:
    registry: ""
    repository: kserve/storage-initializer
    tag: v0.8.0
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1000m
      memory: 1Gi
# predictor spec configs for inferenceservices
predictors:
  tensorflow:
    image: tensorflow/serving
    defaultImageVersion: 2.6.2
    defaultGpuImageVersion: 2.6.2-gpu
    defaultTimeout: "60"
    supportedFrameworks:
      - tensorflow
    multiModelServer: false
  onnx:
    image: mcr.microsoft.com/onnxruntime/server
    defaultImageVersion: v1.0.0
    supportedFrameworks:
      - onnx
    multiModelServer: false
  sklearn:
    v1:
      image: kserve/sklearnserver
      defaultImageVersion: v0.8.0
      supportedFrameworks:
        - sklearn
      multiModelServer: true
    v2:
      image: docker.io/seldonio/mlserver
      defaultImageVersion: 0.5.3
      supportedFrameworks:
        - sklearn
      multiModelServer: true
  xgboost:
    v1:
      image: kserve/xgbserver
      defaultImageVersion: v0.8.0
      supportedFrameworks:
        - xgboost
      multiModelServer: true
    v2:
      image: docker.io/seldonio/mlserver
      defaultImageVersion: 0.5.3
      supportedFrameworks:
        - xgboost
      multiModelServer: true
  pytorch:
    v1:
      image: kserve/torchserve-kfs
      defaultImageVersion: 0.5.3
      defaultGpuImageVersion: 0.5.3-gpu
      supportedFrameworks:
        - pytorch
      multiModelServer: false
    v2:
      image: kserve/torchserve-kfs
      defaultImageVersion: 0.5.3
      defaultGpuImageVersion: 0.5.3-gpu
      supportedFrameworks:
        - pytorch
      multiModelServer: false
  triton:
    image: nvcr.io/nvidia/tritonserver
    defaultImageVersion: 21.09-py3
    supportedFrameworks:
      - tensorrt
      - tensorflow
      - onnx
      - pytorch
    multiModelServer: true
  pmml:
    image: kserve/pmmlserver
    defaultImageVersion: v0.8.0
    supportedFrameworks:
      - pmml
    multiModelServer: false
  lightgbm:
    image: kserve/lgbserver
    defaultImageVersion: v0.8.0
    supportedFrameworks:
      - lightgbm
    multiModelServer: false
  paddle:
    image: kserve/paddleserver
    defaultImageVersion: v0.8.0
    supportedFrameworks:
      - paddle
    multiModelServer: false
# clusterServingRuntimes contains specs for each predictor in predictors above
clusterServingRuntimes:
  - name: kserve-lgbserver
    spec:
      containers:
        - args:
            - --model_name={{.Name}}
            - --model_dir=/mnt/models
            - --http_port=8080
            - --nthread=1
          image: kserve/lgbserver:v0.8.0
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: lightgbm
          version: "2"
  - name: kserve-mlserver
    spec:
      containers:
        - env:
            - name: MLSERVER_MODEL_IMPLEMENTATION
              value: "{{.Labels.modelClass}}"
            - name: MLSERVER_HTTP_PORT
              value: "8080"
            - name: MLSERVER_GRPC_PORT
              value: "9000"
            - name: MODELS_DIR
              value: /mnt/models
          image: docker.io/seldonio/mlserver:0.5.3
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - name: sklearn
          version: "0"
        - name: xgboost
          version: "1"
        - name: lightgbm
          version: "3"
        - autoSelect: true
          name: mlflow
          version: "1"
  - name: kserve-paddleserver
    spec:
      containers:
        - args:
            - --model_name={{.Name}}
            - --model_dir=/mnt/models
            - --http_port=8080
          image: kserve/paddleserver:v0.8.0
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: paddle
          version: "2"
  - name: kserve-pmmlserver
    spec:
      containers:
        - args:
            - --model_name={{.Name}}
            - --model_dir=/mnt/models
            - --http_port=8080
          image: kserve/pmmlserver:v0.8.0
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: pmml
          version: "3"
        - autoSelect: true
          name: pmml
          version: "4"
  - name: kserve-sklearnserver
    spec:
      containers:
        - args:
            - --model_name={{.Name}}
            - --model_dir=/mnt/models
            - --http_port=8080
          image: kserve/sklearnserver:v0.8.0
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: sklearn
          version: "1"
  - name: kserve-tensorflow-serving
    spec:
      containers:
        - args:
            - --model_name={{.Name}}
            - --port=9000
            - --rest_api_port=8080
            - --model_base_path=/mnt/models
            - --rest_api_timeout_in_ms=60000
          command:
            - /usr/bin/tensorflow_model_server
          image: tensorflow/serving:2.6.2
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: tensorflow
          version: "1"
        - autoSelect: true
          name: tensorflow
          version: "2"
  - name: kserve-torchserve
    spec:
      containers:
        - args:
            - torchserve
            - --start
            - --model-store=/mnt/models/model-store
            - --ts-config=/mnt/models/config/config.properties
          env:
            - name: TS_SERVICE_ENVELOPE
              value: "{{.Labels.serviceEnvelope}}"
          image: kserve/torchserve-kfs:0.5.3
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: pytorch
          version: "1"
  - name: kserve-tritonserver
    spec:
      containers:
        - args:
            - tritonserver
            - --model-store=/mnt/models
            - --grpc-port=9000
            - --http-port=8080
            - --allow-grpc=true
            - --allow-http=true
          image: nvcr.io/nvidia/tritonserver:21.09-py3
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - name: tensorrt
          version: "8"
        - name: tensorflow
          version: "1"
        - name: tensorflow
          version: "2"
        - autoSelect: true
          name: onnx
          version: "1"
        - name: pytorch
          version: "1"
        - autoSelect: true
          name: triton
          version: "2"
  - name: kserve-xgbserver
    spec:
      containers:
        - args:
            - --model_name={{.Name}}
            - --model_dir=/mnt/models
            - --http_port=8080
            - --nthread=1
          image: kserve/xgbserver:v0.8.0
          name: kserve-container
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
      supportedModelFormats:
        - autoSelect: true
          name: xgboost
          version: "1"
###################################################################
# knativeServingIstio dependency configuration
# #################################################################
knativeServingIstio:
  enabled: true
  helmChart:
    chart: knative-serving-istio
    version: 1.7.1
    repository: "https://caraml-dev.github.io/helm-charts"
    release: knative-serving
    namespace: knative-serving
    createNamespace: true
###################################################################
# Cert manager Base for Cert Manager CRDs
# #################################################################
certManagerBase:
  enabled: true
###################################################################
# Cert manager dependency
# #################################################################
# Use hyphen so chart name can be used as part of resource name
cert-manager:
  fullnameOverride: cert-manager
  enabled: true

# NOTE: We do not enable crds because the crds are installed in separate helm chart
# installCRDs: true
