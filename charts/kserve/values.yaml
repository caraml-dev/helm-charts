# Default values for kserve.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Specify how inference service is deployed:
# - Serverless, use Knative
# - RawDeployment, use K8S Deployment
defaultDeploymentMode: Serverless

controller:
  image:
    registry: ""
    repository: kserve/kserve-controller
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v0.8.0"

  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  nodeSelector: {}
  tolerations: []
  affinity: {}

ingress:
  ingressGateway: knative-serving/knative-ingress-gateway
  ingressService: istio-ingressgateway.istio-system.svc.cluster.local
  localGateway: knative-serving/knative-local-gateway
  localGatewayService: cluster-local-gateway.istio-system.svc.cluster.local
  ingressDomain: example.com
  ingressClassName: istio
  domainTemplate: "{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}"

logger:
  image:
    registry: ""
    repository: kserve/agent
    tag: v0.8.0
  resources:
    limits:
      cpu: 100m
      memory: 100Mi
    requests:
      cpu: 1
      memory: 1Gi
  defaultUrl: http://default-broker

agent:
  image:
    registry: ""
    repository: kserve/agent
    tag: v0.8.0
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 100Mi

batcher:
  image:
    registry: ""
    repository: kserve/agent
    tag: v0.8.0
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 1
      memory: 1Gi

storageInitializer:
  image:
    registry: ""
    repository: kserve/storage-initializer
    tag: v0.8.0
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 100Mi

predictors:
  tensorflow:
    image: tensorflow/serving
    defaultImageVersion: 2.6.2
    defaultGpuImageVersion: 2.6.2-gpu
    defaultTimeout: '60'
    supportedFrameworks:
    - tensorflow
    multiModelServer: false
  onnx:
    image: mcr.microsoft.com/onnxruntime/server
    defaultImageVersion: v1.0.0
    supportedFrameworks:
    - onnx
    multiModelServer: false
  sklearn:
    v1:
      image: kserve/sklearnserver
      defaultImageVersion: v0.8.0
      supportedFrameworks:
      - sklearn
      multiModelServer: true
    v2:
      image: docker.io/seldonio/mlserver
      defaultImageVersion: 0.5.3
      supportedFrameworks:
      - sklearn
      multiModelServer: true
  xgboost:
    v1:
      image: kserve/xgbserver
      defaultImageVersion: v0.8.0
      supportedFrameworks:
      - xgboost
      multiModelServer: true
    v2:
      image: docker.io/seldonio/mlserver
      defaultImageVersion: 0.5.3
      supportedFrameworks:
      - xgboost
      multiModelServer: true
  pytorch:
    v1:
      image: kserve/torchserve-kfs
      defaultImageVersion: 0.5.3
      defaultGpuImageVersion: 0.5.3-gpu
      supportedFrameworks:
      - pytorch
      multiModelServer: false
    v2:
      image: kserve/torchserve-kfs
      defaultImageVersion: 0.5.3
      defaultGpuImageVersion: 0.5.3-gpu
      supportedFrameworks:
      - pytorch
      multiModelServer: false
  triton:
    image: nvcr.io/nvidia/tritonserver
    defaultImageVersion: 21.09-py3
    supportedFrameworks:
    - tensorrt
    - tensorflow
    - onnx
    - pytorch
    multiModelServer: true
  pmml:
    image: kserve/pmmlserver
    defaultImageVersion: v0.8.0
    supportedFrameworks:
    - pmml
    multiModelServer: false
  lightgbm:
    image: kserve/lgbserver
    defaultImageVersion: v0.8.0
    supportedFrameworks:
    - lightgbm
    multiModelServer: false
  paddle:
    image: kserve/paddleserver
    defaultImageVersion: v0.8.0
    supportedFrameworks:
    - paddle
    multiModelServer: false


clusterServingRuntimes:
  - name: kserve-lgbserver
    spec:
      containers:
      - args:
        - --model_name={{.Name}}
        - --model_dir=/mnt/models
        - --http_port=8080
        - --nthread=1
        image: kserve/lgbserver:v0.8.0
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: lightgbm
        version: "2"
  - name: kserve-mlserver
    spec:
      containers:
      - env:
        - name: MLSERVER_MODEL_IMPLEMENTATION
          value: '{{.Labels.modelClass}}'
        - name: MLSERVER_HTTP_PORT
          value: "8080"
        - name: MLSERVER_GRPC_PORT
          value: "9000"
        - name: MODELS_DIR
          value: /mnt/models
        image: docker.io/seldonio/mlserver:0.5.3
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - name: sklearn
        version: "0"
      - name: xgboost
        version: "1"
      - name: lightgbm
        version: "3"
      - autoSelect: true
        name: mlflow
        version: "1"
  - name: kserve-paddleserver
    spec:
      containers:
      - args:
        - --model_name={{.Name}}
        - --model_dir=/mnt/models
        - --http_port=8080
        image: kserve/paddleserver:v0.8.0
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: paddle
        version: "2"
  - name: kserve-pmmlserver
    spec:
      containers:
      - args:
        - --model_name={{.Name}}
        - --model_dir=/mnt/models
        - --http_port=8080
        image: kserve/pmmlserver:v0.8.0
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: pmml
        version: "3"
      - autoSelect: true
        name: pmml
        version: "4"
  - name: kserve-sklearnserver
    spec:
      containers:
      - args:
        - --model_name={{.Name}}
        - --model_dir=/mnt/models
        - --http_port=8080
        image: kserve/sklearnserver:v0.8.0
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: sklearn
        version: "1"
  - name: kserve-tensorflow-serving
    spec:
      containers:
      - args:
        - --model_name={{.Name}}
        - --port=9000
        - --rest_api_port=8080
        - --model_base_path=/mnt/models
        - --rest_api_timeout_in_ms=60000
        command:
        - /usr/bin/tensorflow_model_server
        image: tensorflow/serving:2.6.2
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: tensorflow
        version: "1"
      - autoSelect: true
        name: tensorflow
        version: "2"
  - name: kserve-torchserve
    spec:
      containers:
      - args:
        - torchserve
        - --start
        - --model-store=/mnt/models/model-store
        - --ts-config=/mnt/models/config/config.properties
        env:
        - name: TS_SERVICE_ENVELOPE
          value: '{{.Labels.serviceEnvelope}}'
        image: kserve/torchserve-kfs:0.5.3
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: pytorch
        version: "1"
  - name: kserve-tritonserver
    spec:
      containers:
      - args:
        - tritonserver
        - --model-store=/mnt/models
        - --grpc-port=9000
        - --http-port=8080
        - --allow-grpc=true
        - --allow-http=true
        image: nvcr.io/nvidia/tritonserver:21.09-py3
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - name: tensorrt
        version: "8"
      - name: tensorflow
        version: "1"
      - name: tensorflow
        version: "2"
      - autoSelect: true
        name: onnx
        version: "1"
      - name: pytorch
        version: "1"
      - autoSelect: true
        name: triton
        version: "2"
  - name: kserve-xgbserver
    spec:
      containers:
      - args:
        - --model_name={{.Name}}
        - --model_dir=/mnt/models
        - --http_port=8080
        - --nthread=1
        image: kserve/xgbserver:v0.8.0
        name: kserve-container
        resources:
          limits:
            cpu: "1"
            memory: 2Gi
          requests:
            cpu: "1"
            memory: 2Gi
      supportedModelFormats:
      - autoSelect: true
        name: xgboost
        version: "1"


###################################################################
# knativeServingIstio dependency configuration
# #################################################################
knativeServingIstio:
  enabled: true
  helmChart:
    chart: knative-serving-istio
    version: 1.0.2
    repository: "https://caraml-dev.github.io/helm-charts"
    release: knative-serving
    namespace: knative-serving
    createNamespace: true


###################################################################
# Cert manager Base for Cert Manager CRDs
# #################################################################
certManagerBase:
  enabled: true

###################################################################
# Cert manager dependency
# #################################################################
# Use hyphen so chart name can be used as part of resource name
cert-manager:
  fullnameOverride: cert-manager
  enabled: true

  # NOTE: We do not enable crds because the crds are installed in separate helm chart
  # installCRDs: true
